{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Web Scraping Tables using Pandas\n",
        "========================================\n",
        "\n",
        "**Estimated Effort: 5 mins**\n",
        "\n",
        "The Pandas library in Python contains a function `read_html()` that can be used to extract tabular information from any web page.\n",
        "\n",
        "This notebook covers:\n",
        "- Using `pandas.read_html()` to extract tables\n",
        "- Working with multiple tables on a webpage\n",
        "- Handling limitations and data cleaning\n",
        "- Practical examples with real websites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Required Libraries\n",
        "\n",
        "Let's start by importing the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install pandas lxml html5lib beautifulsoup4\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Extracting Largest Banks Table\n",
        "\n",
        "Let us assume we want to extract the list of the largest banks in the world by market capitalization, from the following link:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the URL for largest banks\n",
        "URL = 'https://en.wikipedia.org/wiki/List_of_largest_banks'\n",
        "\n",
        "print(f\"Target URL: {URL}\")\n",
        "print(\"We will extract the table showing the largest banks by market capitalization.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may use `pandas.read_html()` function in python to extract all the tables in the web page directly.\n",
        "\n",
        "**Note:** This is a live web page and it may get updated over time. The process of data extraction remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all tables from the webpage\n",
        "try:\n",
        "    tables = pd.read_html(URL)\n",
        "    print(f\"Successfully extracted {len(tables)} tables from the webpage\")\n",
        "    \n",
        "    # Display information about each table\n",
        "    for i, table in enumerate(tables):\n",
        "        print(f\"Table {i}: Shape {table.shape} (rows, columns)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error extracting tables: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the first table (index 0) which contains the largest banks\n",
        "df = tables[0]\n",
        "\n",
        "print(\"Largest Banks Table:\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(\"\\nFirst 10 rows:\")\n",
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information about the dataframe\n",
        "print(\"DataFrame Info:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Countries by GDP Table\n",
        "\n",
        "Let's look at another example. Consider the following URL showing the list of countries by GDP (nominal):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the URL for countries by GDP\n",
        "URL = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'\n",
        "\n",
        "print(f\"Target URL: {URL}\")\n",
        "print(\"This page contains multiple tables with GDP data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all tables from the GDP webpage\n",
        "try:\n",
        "    tables = pd.read_html(URL)\n",
        "    print(f\"Successfully extracted {len(tables)} tables from the webpage\")\n",
        "    \n",
        "    # Display detailed information about each table\n",
        "    for i, table in enumerate(tables):\n",
        "        print(f\"\\nTable {i}:\")\n",
        "        print(f\"  Shape: {table.shape} (rows, columns)\")\n",
        "        print(f\"  Columns: {list(table.columns)[:5]}...\")  # Show first 5 columns\n",
        "        \n",
        "        # Show a sample of data\n",
        "        if table.shape[0] > 0:\n",
        "            print(f\"  Sample data: {table.iloc[0, 0] if not pd.isna(table.iloc[0, 0]) else 'N/A'}\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"Error extracting tables: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important Note:** The contents of the tables in web pages may contain elements such as hyperlink text and other denoters, which are also scraped directly using the pandas method. This may lead to a requirement of further cleaning of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the required table (index 2 in the original example)\n",
        "# Note: Table indices may change as the webpage is updated\n",
        "try:\n",
        "    # Try different table indices to find the main GDP table\n",
        "    for i in range(min(5, len(tables))):\n",
        "        table = tables[i]\n",
        "        if table.shape[0] > 10 and table.shape[1] >= 3:  # Look for substantial tables\n",
        "            # Check if it contains country data\n",
        "            first_col = str(table.iloc[0, 0]).lower()\n",
        "            if any(keyword in first_col for keyword in ['country', 'rank', 'united', 'china']):\n",
        "                print(f\"Found main GDP table at index {i}\")\n",
        "                df_gdp = table\n",
        "                break\n",
        "    else:\n",
        "        # Fallback to table 2 as in the original example\n",
        "        df_gdp = tables[2]\n",
        "        print(\"Using table index 2 (original example)\")\n",
        "    \n",
        "    print(f\"\\nGDP Table Shape: {df_gdp.shape}\")\n",
        "    print(f\"Columns: {list(df_gdp.columns)}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error accessing GDP table: {e}\")\n",
        "    df_gdp = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the GDP table\n",
        "if df_gdp is not None:\n",
        "    print(\"Countries by GDP (Nominal) - First 15 rows:\")\n",
        "    print(df_gdp.head(15))\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Data Types:\")\n",
        "    print(df_gdp.dtypes)\nelse:\n",
        "    print(\"GDP table not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitations and Data Cleaning\n",
        "\n",
        "Although convenient, the `pandas.read_html()` method comes with its own set of limitations:\n",
        "\n",
        "1. **Hidden Tables**: Web pages may have content saved as tables but they may not appear as tables visually\n",
        "2. **Extra Content**: Tables may contain hyperlinks, images, and other HTML elements\n",
        "3. **Formatting Issues**: Data may need cleaning due to embedded HTML content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of data cleaning - removing unwanted characters\n",
        "if df_gdp is not None and df_gdp.shape[0] > 0:\n",
        "    # Create a copy for cleaning\n",
        "    df_cleaned = df_gdp.copy()\n",
        "    \n",
        "    # Show original data\n",
        "    print(\"Original data (first few rows):\")\n",
        "    print(df_gdp.head(3))\n",
        "    \n",
        "    # Clean string columns by removing common Wikipedia artifacts\n",
        "    for col in df_cleaned.columns:\n",
        "        if df_cleaned[col].dtype == 'object':\n",
        "            # Remove common Wikipedia reference markers\n",
        "            df_cleaned[col] = df_cleaned[col].astype(str).str.replace(r'\\[.*?\\]', '', regex=True)\n",
        "            df_cleaned[col] = df_cleaned[col].str.replace(r'\\(.*?\\)', '', regex=True)\n",
        "            df_cleaned[col] = df_cleaned[col].str.strip()\n",
        "    \n",
        "    print(\"\\nCleaned data (first few rows):\")\n",
        "    print(df_cleaned.head(3))\n",
        "    \n",
        "    print(\"\\nCleaning completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Usage Tips\n",
        "\n",
        "Here are some additional tips for using `pandas.read_html()` effectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced usage with parameters\n",
        "def extract_tables_advanced(url, match=None, header=0):\n",
        "    \"\"\"\n",
        "    Extract tables with advanced parameters\n",
        "    \n",
        "    Parameters:\n",
        "    - url: Target URL\n",
        "    - match: String or regex to match specific tables\n",
        "    - header: Row to use as column headers\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use additional parameters for better control\n",
        "        tables = pd.read_html(\n",
        "            url,\n",
        "            match=match,           # Find tables containing specific text\n",
        "            header=header,         # Specify header row\n",
        "            skiprows=None,         # Skip rows if needed\n",
        "            attrs=None,            # HTML attributes to match\n",
        "            parse_dates=False,     # Parse date columns\n",
        "            thousands=',',         # Thousands separator\n",
        "            encoding=None,         # Character encoding\n",
        "            decimal='.',           # Decimal separator\n",
        "            converters=None,       # Column converters\n",
        "            na_values=None,        # Additional NA values\n",
        "            keep_default_na=True   # Keep default NA values\n",
        "        )\n",
        "        \n",
        "        return tables\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "# Example usage\n",
        "print(\"Advanced table extraction example:\")\n",
        "\n",
        "# Try to find tables containing \"Bank\" in the banks webpage\n",
        "try:\n",
        "    bank_tables = extract_tables_advanced(\n",
        "        'https://en.wikipedia.org/wiki/List_of_largest_banks',\n",
        "        match='Bank'\n",
        "    )\n",
        "    print(f\"Found {len(bank_tables)} tables containing 'Bank'\")\nexcept:\n",
        "    print(\"Advanced extraction failed, using basic method\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Exercise: Extract and Analyze Data\n",
        "\n",
        "Let's practice with a complete workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete workflow example\n",
        "def scrape_and_analyze_table(url, table_index=0, save_csv=False):\n",
        "    \"\"\"\n",
        "    Complete workflow for table scraping and basic analysis\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Extract tables\n",
        "        print(f\"Extracting tables from: {url}\")\n",
        "        tables = pd.read_html(url)\n",
        "        print(f\"Found {len(tables)} tables\")\n",
        "        \n",
        "        # Step 2: Select and examine table\n",
        "        if table_index >= len(tables):\n",
        "            print(f\"Table index {table_index} not available. Using index 0.\")\n",
        "            table_index = 0\n",
        "        \n",
        "        df = tables[table_index]\n",
        "        print(f\"\\nSelected table {table_index}: Shape {df.shape}\")\n",
        "        \n",
        "        # Step 3: Basic cleaning\n",
        "        df_clean = df.copy()\n",
        "        \n",
        "        # Clean string columns\n",
        "        for col in df_clean.select_dtypes(include=['object']).columns:\n",
        "            df_clean[col] = df_clean[col].astype(str).str.replace(r'\\[.*?\\]', '', regex=True)\n",
        "            df_clean[col] = df_clean[col].str.strip()\n",
        "        \n",
        "        # Step 4: Basic analysis\n",
        "        print(f\"\\nColumns: {list(df_clean.columns)}\")\n",
        "        print(f\"Missing values: {df_clean.isnull().sum().sum()}\")\n",
        "        print(f\"Duplicate rows: {df_clean.duplicated().sum()}\")\n",
        "        \n",
        "        # Step 5: Display sample\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(df_clean.head())\n",
        "        \n",
        "        # Step 6: Save if requested\n",
        "        if save_csv:\n",
        "            filename = f\"scraped_table_{table_index}.csv\"\n",
        "            df_clean.to_csv(filename, index=False)\n",
        "            print(f\"\\nTable saved as: {filename}\")\n",
        "        \n",
        "        return df_clean\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in workflow: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test the workflow\n",
        "result = scrape_and_analyze_table(\n",
        "    'https://en.wikipedia.org/wiki/List_of_largest_banks',\n",
        "    table_index=0,\n",
        "    save_csv=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling and Best Practices\n",
        "\n",
        "When scraping tables, it's important to handle potential errors gracefully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Robust table scraping function\n",
        "def robust_table_scraper(url, max_retries=3, timeout=30):\n",
        "    \"\"\"\n",
        "    Robust table scraping with error handling\n",
        "    \"\"\"\n",
        "    import time\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Attempt {attempt + 1}/{max_retries}\")\n",
        "            \n",
        "            # Add delay between retries\n",
        "            if attempt > 0:\n",
        "                time.sleep(2)\n",
        "            \n",
        "            # Try to read tables with timeout\n",
        "            tables = pd.read_html(url, io=None)\n",
        "            \n",
        "            if tables:\n",
        "                print(f\"Success! Found {len(tables)} tables\")\n",
        "                return tables\n",
        "            else:\n",
        "                print(\"No tables found\")\n",
        "                \n",
        "        except ImportError as e:\n",
        "            print(f\"Missing dependency: {e}\")\n",
        "            print(\"Try: pip install lxml html5lib beautifulsoup4\")\n",
        "            break\n",
        "            \n",
        "        except ValueError as e:\n",
        "            print(f\"No tables found: {e}\")\n",
        "            break\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            \n",
        "    print(\"All attempts failed\")\n",
        "    return []\n",
        "\n",
        "# Test robust scraper\n",
        "print(\"Testing robust table scraper:\")\n",
        "robust_tables = robust_table_scraper('https://en.wikipedia.org/wiki/List_of_largest_banks')\n",
        "\n",
        "if robust_tables:\n",
        "    print(f\"Successfully extracted {len(robust_tables)} tables\")\nelse:\n",
        "    print(\"Failed to extract tables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, we learned how to:\n",
        "\n",
        "1. **Extract tables** using `pandas.read_html()`\n",
        "2. **Handle multiple tables** on a single webpage\n",
        "3. **Clean scraped data** by removing HTML artifacts\n",
        "4. **Use advanced parameters** for better control\n",
        "5. **Implement error handling** for robust scraping\n",
        "6. **Save results** to CSV files for further analysis\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "- `pandas.read_html()` is powerful but may need data cleaning\n",
        "- Always inspect table structure before analysis\n",
        "- Handle errors gracefully with try-except blocks\n",
        "- Web pages change over time - code may need updates\n",
        "- Consider using additional parameters for better results\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Learn about BeautifulSoup for more complex scraping\n",
        "- Explore Selenium for dynamic content\n",
        "- Practice with different websites and table structures\n",
        "- Implement data validation and quality checks"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}