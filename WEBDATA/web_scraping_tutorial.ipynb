{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Web Scraping with Python\n",
          "========================================\n",
          "\n",
          "Web scraping involves extracting information from web pages using Python. It can save time and automate data collection.\n",
          "\n",
          "This notebook covers:\n",
          "- Setting up required tools (Requests and Beautiful Soup)\n",
          "- Fetching and parsing HTML content\n",
          "- Navigating HTML structure\n",
          "- Custom data extraction\n",
          "- Using pandas for table extraction"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Required Tools\n",
          "\n",
          "Web scraping requires Python code and two essential modules: **Requests** and **Beautiful Soup**. Ensure you have both modules installed in your Python environment."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Install required packages if needed\n",
          "# !pip install requests beautifulsoup4 pandas lxml html5lib\n",
          "\n",
          "# Import required libraries\n",
          "import requests\n",
          "from bs4 import BeautifulSoup\n",
          "import pandas as pd\n",
          "import time\n",
          "\n",
          "print(\"All required libraries imported successfully!\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Fetching and Parsing HTML\n",
          "\n",
          "To start web scraping, you need to fetch the HTML content of a webpage and parse it using Beautiful Soup. Here's a step-by-step example:"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Specify the URL of the webpage you want to scrape\n",
          "url = 'https://en.wikipedia.org/wiki/IBM'\n",
          "\n",
          "# Send an HTTP GET request to the webpage\n",
          "response = requests.get(url)\n",
          "\n",
          "# Check if the request was successful\n",
          "if response.status_code == 200:\n",
          "    print(f\"Successfully fetched the webpage! Status code: {response.status_code}\")\n",
          "else:\n",
          "    print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n",
          "\n",
          "# Store the HTML content in a variable\n",
          "html_content = response.text\n",
          "\n",
          "# Create a BeautifulSoup object to parse the HTML\n",
          "soup = BeautifulSoup(html_content, 'html.parser')\n",
          "\n",
          "# Display a snippet of the HTML content\n",
          "print(\"\\nFirst 500 characters of HTML content:\")\n",
          "print(html_content[:500])"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Navigating the HTML Structure\n",
          "\n",
          "BeautifulSoup represents HTML content as a tree-like structure, allowing for easy navigation. You can use methods like `find_all` to filter and extract specific HTML elements."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Find the page title\n",
          "title = soup.find('title')\n",
          "print(f\"Page title: {title.text}\")\n",
          "\n",
          "# Find the main heading (h1)\n",
          "main_heading = soup.find('h1')\n",
          "if main_heading:\n",
          "    print(f\"Main heading: {main_heading.text}\")\n",
          "\n",
          "# Find all headings (h1, h2, h3)\n",
          "headings = soup.find_all(['h1', 'h2', 'h3'])\n",
          "print(f\"\\nFound {len(headings)} headings on the page\")\n",
          "\n",
          "# Display first 5 headings\n",
          "print(\"\\nFirst 5 headings:\")\n",
          "for i, heading in enumerate(headings[:5]):\n",
          "    print(f\"{i+1}. {heading.name}: {heading.text.strip()}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Find all <a> tags (anchor tags) in the HTML\n",
          "links = soup.find_all('a')\n",
          "\n",
          "print(f\"Found {len(links)} links on the page\")\n",
          "\n",
          "# Iterate through the first 10 links and print their text and href\n",
          "print(\"\\nFirst 10 links:\")\n",
          "for i, link in enumerate(links[:10]):\n",
          "    link_text = link.text.strip()\n",
          "    link_href = link.get('href', 'No href')\n",
          "    \n",
          "    # Only show links with meaningful text\n",
          "    if link_text:\n",
          "        print(f\"{i+1}. Text: '{link_text}' | URL: {link_href}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Custom Data Extraction\n",
          "\n",
          "Web scraping allows you to navigate the HTML structure and extract specific information based on your requirements. This process may involve finding specific tags, attributes, or text content within the HTML document."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Extract specific information from the Wikipedia page\n",
          "\n",
          "# Find the first paragraph of the article\n",
          "first_paragraph = soup.find('div', class_='mw-parser-output').find('p')\n",
          "if first_paragraph:\n",
          "    print(\"First paragraph of the article:\")\n",
          "    print(first_paragraph.text.strip()[:300] + \"...\")\n",
          "\n",
          "# Find the infobox (if it exists)\n",
          "infobox = soup.find('table', class_='infobox')\n",
          "if infobox:\n",
          "    print(\"\\n\" + \"=\"*50)\n",
          "    print(\"INFOBOX DATA\")\n",
          "    print(\"=\"*50)\n",
          "    \n",
          "    # Extract key-value pairs from infobox\n",
          "    rows = infobox.find_all('tr')\n",
          "    for row in rows[:10]:  # First 10 rows\n",
          "        cells = row.find_all(['th', 'td'])\n",
          "        if len(cells) == 2:\n",
          "            key = cells[0].text.strip()\n",
          "            value = cells[1].text.strip()\n",
          "            if key and value:\n",
          "                print(f\"{key}: {value[:100]}\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Extract all images and their descriptions\n",
          "images = soup.find_all('img')\n",
          "\n",
          "print(f\"Found {len(images)} images on the page\")\n",
          "print(\"\\nFirst 5 images with descriptions:\")\n",
          "\n",
          "for i, img in enumerate(images[:5]):\n",
          "    src = img.get('src', 'No source')\n",
          "    alt = img.get('alt', 'No description')\n",
          "    print(f\"{i+1}. Source: {src}\")\n",
          "    print(f\"   Description: {alt}\")\n",
          "    print()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Using BeautifulSoup for Advanced HTML Parsing\n",
          "\n",
          "Beautiful Soup is a powerful tool for navigating and extracting specific web page parts. It allows you to find elements based on their tags, attributes, or text, making it easier to extract the information you're interested in."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Advanced BeautifulSoup techniques\n",
          "\n",
          "# 1. Find elements by CSS selectors\n",
          "print(\"Using CSS selectors:\")\n",
          "css_headings = soup.select('h2')\n",
          "print(f\"Found {len(css_headings)} h2 elements using CSS selector\")\n",
          "\n",
          "# 2. Find elements by attribute\n",
          "print(\"\\nFinding elements by attribute:\")\n",
          "elements_with_id = soup.find_all(attrs={'id': True})\n",
          "print(f\"Found {len(elements_with_id)} elements with 'id' attribute\")\n",
          "\n",
          "# 3. Find elements containing specific text\n",
          "print(\"\\nFinding elements containing 'IBM':\")\n",
          "ibm_elements = soup.find_all(text=lambda text: text and 'IBM' in text)\n",
          "print(f\"Found {len(ibm_elements)} text elements containing 'IBM'\")\n",
          "\n",
          "# Show first 3 examples\n",
          "for i, text in enumerate(ibm_elements[:3]):\n",
          "    clean_text = text.strip()\n",
          "    if clean_text:\n",
          "        print(f\"{i+1}. {clean_text[:100]}...\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Extract structured data: all external links\n",
          "print(\"External links from the page:\")\n",
          "print(\"=\"*40)\n",
          "\n",
          "external_links = []\n",
          "for link in soup.find_all('a', href=True):\n",
          "    href = link['href']\n",
          "    # Check if it's an external link (starts with http)\n",
          "    if href.startswith('http'):\n",
          "        link_text = link.text.strip()\n",
          "        if link_text:  # Only include links with text\n",
          "            external_links.append({\n",
          "                'text': link_text,\n",
          "                'url': href\n",
          "            })\n",
          "\n",
          "# Remove duplicates and show first 10\n",
          "unique_links = {link['url']: link for link in external_links}\n",
          "for i, (url, link_data) in enumerate(list(unique_links.items())[:10]):\n",
          "    print(f\"{i+1}. {link_data['text'][:50]}\")\n",
          "    print(f\"   URL: {url}\")\n",
          "    print()"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Using Pandas read_html for Table Extraction\n",
          "\n",
          "Pandas, a Python library, provides a function called `read_html`, which can automatically extract data from websites' tables and present it in a format suitable for analysis. It's similar to taking a table from a webpage and importing it into a spreadsheet for further analysis."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Use pandas to extract tables from the webpage\n",
          "try:\n",
          "    # Extract all tables from the webpage\n",
          "    tables = pd.read_html(url)\n",
          "    \n",
          "    print(f\"Found {len(tables)} tables on the webpage\")\n",
          "    \n",
          "    # Display information about each table\n",
          "    for i, table in enumerate(tables):\n",
          "        print(f\"\\nTable {i+1}:\")\n",
          "        print(f\"  Shape: {table.shape} (rows, columns)\")\n",
          "        print(f\"  Columns: {list(table.columns)}\")\n",
          "        \n",
          "        # Show first few rows if the table has reasonable size\n",
          "        if table.shape[0] > 0 and table.shape[1] <= 10:\n",
          "            print(f\"  First few rows:\")\n",
          "            print(table.head(3).to_string(index=False))\n",
          "        print(\"-\" * 50)\n",
          "\n",
          "except Exception as e:\n",
          "    print(f\"Error extracting tables: {e}\")\n",
          "    print(\"This might happen if no tables are found or if there are parsing issues.\")"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Let's try a different URL with more obvious tables\n",
          "# Wikipedia's list of countries by population\n",
          "table_url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
          "\n",
          "try:\n",
          "    print(\"Extracting tables from countries by population page...\")\n",
          "    population_tables = pd.read_html(table_url)\n",
          "    \n",
          "    print(f\"Found {len(population_tables)} tables\")\n",
          "    \n",
          "    # The main table is usually the first one\n",
          "    if len(population_tables) > 0:\n",
          "        main_table = population_tables[0]\n",
          "        print(f\"\\nMain table shape: {main_table.shape}\")\n",
          "        print(f\"Columns: {list(main_table.columns)}\")\n",
          "        \n",
          "        # Display top 10 countries by population\n",
          "        print(\"\\nTop 10 entries:\")\n",
          "        print(main_table.head(10).to_string(index=False))\n",
          "        \n",
          "        # Save the table to CSV for further analysis\n",
          "        main_table.to_csv('countries_population.csv', index=False)\n",
          "        print(\"\\nTable saved as 'countries_population.csv'\")\n",
          "\n",
          "except Exception as e:\n",
          "    print(f\"Error extracting population tables: {e}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Best Practices and Ethics\n",
          "\n",
          "When web scraping, it's important to follow best practices and ethical guidelines:"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Best practices for web scraping\n",
          "\n",
          "def ethical_scraping_example(url, delay=1):\n",
          "    \"\"\"\n",
          "    Example of ethical web scraping with proper practices\n",
          "    \"\"\"\n",
          "    # 1. Check robots.txt (manually check before scraping)\n",
          "    print(f\"Remember to check: {url}/robots.txt\")\n",
          "    \n",
          "    # 2. Add delay between requests\n",
          "    time.sleep(delay)\n",
          "    \n",
          "    # 3. Use proper headers to identify your bot\n",
          "    headers = {\n",
          "        'User-Agent': 'Educational Web Scraping Bot 1.0 (contact@example.com)'\n",
          "    }\n",
          "    \n",
          "    # 4. Handle errors gracefully\n",
          "    try:\n",
          "        response = requests.get(url, headers=headers, timeout=10)\n",
          "        response.raise_for_status()  # Raises an exception for bad status codes\n",
          "        \n",
          "        print(f\"Successfully scraped: {url}\")\n",
          "        print(f\"Status code: {response.status_code}\")\n",
          "        print(f\"Content length: {len(response.content)} bytes\")\n",
          "        \n",
          "        return response\n",
          "        \n",
          "    except requests.exceptions.RequestException as e:\n",
          "        print(f\"Error scraping {url}: {e}\")\n",
          "        return None\n",
          "\n",
          "# Example usage\n",
          "print(\"Ethical scraping example:\")\n",
          "result = ethical_scraping_example('https://httpbin.org/user-agent')\n",
          "\n",
          "if result:\n",
          "    # Parse the response\n",
          "    data = result.json()\n",
          "    print(f\"\\nServer saw our User-Agent as: {data['user-agent']}\")"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Summary\n",
          "\n",
          "In this notebook, we covered:\n",
          "\n",
          "1. **Setting up tools**: Installing and importing `requests` and `BeautifulSoup`\n",
          "2. **Fetching HTML**: Using `requests.get()` to retrieve webpage content\n",
          "3. **Parsing HTML**: Creating BeautifulSoup objects and navigating the HTML structure\n",
          "4. **Extracting data**: Finding specific elements using various methods\n",
          "5. **Advanced techniques**: CSS selectors, attribute searches, and text filtering\n",
          "6. **Table extraction**: Using `pandas.read_html()` for automatic table parsing\n",
          "7. **Best practices**: Ethical scraping guidelines and error handling\n",
          "\n",
          "### Key Takeaways:\n",
          "- Always respect websites' `robots.txt` and terms of service\n",
          "- Add delays between requests to avoid overwhelming servers\n",
          "- Handle errors gracefully and use appropriate headers\n",
          "- Use the right tool for the job: BeautifulSoup for complex parsing, pandas for tables\n",
          "- Test your scraping code thoroughly before running it at scale"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.8.5"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }